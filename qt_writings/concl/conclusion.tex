% Created 2023-07-26 Wed 10:23
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Filipa  Calado}
\date{\today}
\title{}
\hypersetup{
 pdfauthor={Filipa  Calado},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.2 (Org mode 9.1.9)}, 
 pdflang={English}}
\begin{document}

\tableofcontents

\section{conclusion}
\label{sec:org6243f8e}

\subsection{how to work with LLMs?}
\label{sec:org73e65ae}
\subsubsection{galatea 2.2}
\label{sec:org4b6c4d5}
\begin{itemize}
\item form is random, and so is meaning, pattern matching
\end{itemize}

In the prescient 1995 novel \emph{Galatea 2.2}, by Richard Powers, a
computer is "trained" to generate text in much the same way that an
LLM (Large Language Model) is today: by reading massive amounts. This
computer, which goes by the name of Helen, is "trained" with
literature until she is proficient enough to pass a masters-level
English exam. For the exam's written portion, Helen composes a
poignant post-colonial critique of Caliban's speech from The
Tempest,\footnote{Be not afeard; the isle is full of noises,
Sounds and sweet airs, that give delight, and hurt not.
Sometimes a thousand twangling instruments
Will hum about mine ears; and sometime voices,
That, if I then had waked after long sleep,
Will make me sleep again: and then, in dreaming,
The clouds methought would open, and show riches
Ready to drop upon me; that, when I waked,
I cried to dream again. (III.ii.130–138)} in which she directly addresses her human programmers:
"You are the ones who can hear airs. Who can be frightened or
encouraged. You can hold things and break them and fix them. I never
felt at home here. This is an awful place to be dropped down halfway"
(326).

\subsubsection{current perspectives}
\label{sec:org25c1b85}
Recently, the question about computer intelligence and consciousness
has resurfaced in discussions about LLMs (Large Language Models) and
their derivatives, like Chat-GPT.\footnote{This question of computer "intelligence" has been posed and
answered repeatedly since humans first imagined intelligent machines
half a century ago. One of these imaginations, Alan Turing's, finds
the question of intelligence to be problematic, because there is no
general consensus for what constitutes thinking or feeling in the
first place. Rather, he rephrases the question in his famous " Test"
to one about performance—-to whether or not a computer can verbally
impersonate a human well enough to trick another human into believing
it is intelligent. See Turning, Alan. "Computing Machinery and
Intelligence."} This June, \emph{Critical Inquiry}
pulished a forum on the issue, "Again Theory: A Forum on Language,
Meaning, and Intent in the Time of Stochastic Parrots." Playing on the
title of two influential academic papers, the first from 1982, by
Steven Knapp and Walter Benn Michaels, "Again Theory," and the second
from 2021, by Emily Bender et. Al, "On the Dangers of Stochastic
Parrots: Can Language Models Be Too Big?," the forum explores the role
of \emph{intent} in textual interpretation. For the writers, the question
is a thrilling one--bringing foundational premises for 20th century
literary theory and hermeneutical processes to bear on cutting-edge
so-called "AI" tools being developed today. It seems all of the
training, "authorial intent," "différance," what Ted Underwood in the
forum describes as "the refusal to ground language in an experiencing
subject" has prepared us for this current moment (par. 11).

To the question of whether or not a generated text can be said to have
"intent," most of literary critics here and elsewhere believe that
they cannot, because the LLMs cannot think or feel, at least not
yet. The ability to guess the next word in a sequence, rather than
indicate an underlying intelligence, only indicates an advanced
computer model that has consumed enough data to make accurate
predictions about which word should follow another word.

Other experts in the discipline, however, are reframing the way we
think about this question. Andrew Piper, for example, claims that "the
relationship between language and thought is\ldots{} reversed in a language
model." According to Piper, language may very well be the material
from which concepts like agency and individuality are produced and
constituted. He explains, "Usually we think an entity has wants and
needs and then figures out methods to communicate them. A language
model works the other way round. It has an extensive web of language
and from that emerges a sense of wants and needs."

This view recalls another scene from \emph{Galatea 2.2}, where one of
Helen's programmers, a computer scientist, explains to the other
programmer, a fiction writer, how intelligence works:
\begin{quote}
We humans are winging it, improvising. Input pattern x sets off
associative matrix y, which bears only the slightest relevance to the
stimulus and its often worthless. Conscious intelligence is smoke and
mirrors. Almost free-associative. Nobody really responds to anybody
else, per se. We all spout our canned and thumbnailed scripts, with
the barest minimum of polite segues. Granted, we are remarkably fast
at index and retrieval. 86
\end{quote}
Anybody who has interacted with ChatGPT will know that there is a
certain repetitiveness to its responses. In most cases, the bot's
sentence structure mirrors that of the question. In the above
dialogue, the fiction writer quickly realizes a darker implication
that the computer scientist is making about human intelligence. He
declares, "You're not elevating the machine, you’re debasing us." To
which the computer scientist smugly quips, "Have you read an
undergraduate paper lately?" (86).

Leaving the joke aside, the idea that language is the raw material for
the creation of a subject, that it is the structure through which
subjectivity (however defined) can be said to emerge has been made
over and over by critical theorists.\footnote{See Butler, Judith. "Performative Acts and Gender
Constitution."} According to this view, a
subject does not express thoughts or feelings, but rather, patterns of
thought and feelings are what bring a personality into
being. Following this logic, an LLM might eventually accumulate
something like a personality by spewing massive amounts of text.

\subsubsection{anti-intent}
\label{sec:orgecf7cbd}
But other perspectives on AI “consciousness” exist. In Computational
Linguistics, for example, the work of Emily Bender argues that so
called "Artificial Intelligence" isn't intelligent at all, but just
highly efficient at pattern matching. Bender has an intimate
understanding of how computers process language, having spent her
professional life studying languages and grammars with computational
methods. The real danger of LLMs, which she has been repeating since
her famous paper on "Stochastic Parrots,” is that they concentrate
power, perpetuate systemic discrimination and oppression, and
proliferate disinformation.

According to Bender, is not that technology understands humans, but
that humans misunderstand it. In an earlier paper, "Climbing Towards
NLU," she and her co-author, Alexander Koller, explain that while LLMs
may be adept at "learning" language patterns from processing large
amounts of text data, they do not intuit intent. According to Bender
and Koller, language meaning derives from a combination of expression
and intent, a process summarized by the concise formula: M ⊆ E ×
I. Bender maintains that intent is not contained within word forms,
but is something external, which can only be deduced or imagined in
the mind of the interlocutor. Becuase it is external, intent will
alway remains inaccessible to computers, who are constrained to a
training process that consists of passively processing text. Studies
in language acquisition have already proved such this idea to be
problematic, such as those that test language learning from watching
TV.1 Humans must be active participants in order to construct meaning.

\subsubsection{}
\label{sec:orgf2aeab6}
N. Katherine Hayles, who writes the "Afterword" to the forum, explains
that these programs do have intention, if not because they have
explicit intentions in their programming, but also 

\begin{quote}
They break words into tokens and assign vector locations to them in
positional embedding spaces, which are constructed according to what
other vectors they are related to, as well as their positions within
sentences.  The connections between vectors are expressed as weights,
or parameters, assigned by the program to the different neurons during
training.  Attention and self-attention mechanisms running in parallel
built connections between tokens, according to their syntactic and
semantic correlations. Roughly, the number of parameters indicate how
many connections there are between neurons; in the case of GPT-3, 175
billion; for GPT-4, 170 trillion.  From these vectors, manipulated
through matrix math, the programs construct complex multidimension
maps of vector correlations; the resulting probabilities are run
through a software package such as Softmax that converts them back
into words. par. 2
\end{quote}
Same concept as in chapter one: each word is represented by a list of
numbers that correspond to probabilities of its relation to another
word. This list of numbers is a "vector," because each number
represents a different dimension along which to plot that word in
graphical space. With these models, however, the graph has thousands,
millions, or billions of dimensions, which means it's not visualizable
for humans. This is why matrix multiplication, linear algebra, becomes
important. 


It self-evident to a literary scholar like my self, who has spent the
last three chapters excavating meaning from expressive word forms,
that LLMs.




For us, extrapolating intent is instinctual. We see meaning in
everything. And as anybody who has taken a literature or history class
knows, we especially see meaning in language. 

\subsection{bias}
\label{sec:org0f8d874}
\begin{itemize}
\item who better than humanists to study bias?
\item chiang's "understand," bias in language
\end{itemize}

But this is not the important question. And it is not the question, in
my view, that is going to bring literary studies forward. 

As any humanist scholar knows, are entire academic fields of study
dedicated to different methods of critical analysis, such as
psychoanalysis, to post-structuralism, to queer theory. Excavating
meaning from the traces of history, from texts that might be severed
from an explicit intention or known author, is something that
humanists have been doing for centuries. Underwood suggests that LLMs
might be used for comparative analysis, "not to mimic individual
language understanding, but to represent specific cultural practices
(like styles or expository templates) so they can be studied and
creatively remixed."

I would revise Underwood’s emphasis here: LLMs could offer
opportunities for studying how language encodes and perpetuates bias,
racism, and xenophobia in general. However, the powers who train and
distribute the LLMs are not terribly interested in studying how
language engages with bias, despite their statements to the
contrary. Rather, they are interested in developing products that will
be attractive to everyday people and businesses, such as the numerous
tools already being developed from GPT-4 technology. The problem is
that the race for monetize the tech uncritically reproduces biases
from training set. Models built to ingest as much data as possible, as
quickly as possible, will reproduce only the dominant view.

That being said, if any group of people is equipped to deconstruct the
ways that LLMs work to stifle minority experience, it is precisely
people like Bender and Underwood, who have spent their careers
studying how language creates and perpetuates power structures and
social norms. It is the humanists, especially the ones in cultural and
ethnic studies, who apply lenses from Queer, Black, Chicanx,
Indigenous, and other minority perspectives as frameworks for
analyzing cultural materials.

Despite widespread agreement that AI tools discriminate against
minorities such as women and people of color, solutions for developing
ethical AI often overlook how discrimination begins with seemingly
harmless choices about language.\footnote{See Buolamwini, Joy and Timnit Gebru, "Gender Shades."} Conversations in AI ethics
rarely consider how subtle elements like word choice, tone, and voice
enable and amplify prejudice, and much less how these elements
translate into quantitative representations of words that are computed
by machine learning algorithms. How do large-language models transform
expressions of embodiment, difference, and marginalization into
computable formats?

Language offer rich opportunities for analyzing the ways that bias
adheres to stylistic and formal elements of language. For example, in
a short story by Ted Chiang, the main character’s perspective colors
an almost imperceptible prejudice—a sense of superiority that he holds
over the rest of society. This prejudice, which is exacerbated by the
character’s development of super-intelligence, emerges subtly when he
attempts to analyze everyday social interactions:
\begin{quote}
I walk down the street, watching people go about their business, and
though not a word is spoken, the subtext is conspicuous. A young
couple strolls by, the adoration of one bouncing off the tolerance of
the other. Apprehension flickers and becomes steady as a businessman,
fearful of his supervisor, begins to doubt a decision he made earlier
today. A woman wears a mantle of simulated sophistication, but it
slips when it brushes past the genuine article.
\end{quote}
While his confident tone implies mastery in deciphering interpersonal
dynamics, the character’s word choice also betrays an insecurity—he
sees desperation, doubt, and self-deprecation in the people around
him. Alternatively, what he interprets as desperation in the couple
might also be perceived as unconditional love, and the doubt in the
businessman’s expression might also be read as conscientiousness. This
narrator’s voice here reveals crucial assumptions that limit his
perspective. Bringing these insights to AI, I intend to analyze how
language models are bound by the perspectives in their training data,
as well as the structures and constraints of language itself.



\subsection{interdisciplinarity}
\label{sec:org7eb2d00}
\begin{itemize}
\item to have a debate about the usefulness of LLMs, we need to blend the
disciplines: math and language.
\item finding alignment between formal systems: how does machine learning,
as a formal system, work?
\end{itemize}

\subsection{language/knowledge alone does not move}
\label{sec:org18e781f}
\begin{itemize}
\item sedgwick: what does knowledge do?
\item space break: knowledge isn't enough, language cannot do
\end{itemize}
\subsubsection{space break}
\label{sec:orgbfa1881}

The performative approach to language--the idea that language can
produce meaning seems to hit its own limit at a specific point in the
novel. At this point, the crisis of signification comes to a climax,
when the biographer increasingly drops his pretension toward accuracy
and boldly speculates:
\begin{quote}
‘Shel, my darling,’ she began again, ‘tell me…’ and so they talked two
hours or more, perhaps about Cape Horn, perhaps not, and really it
would profit little to write down what they said, for they knew each
other so well that they could say anything, which is tantamount to
saying nothing, or saying such stupid, prosy things as how to cook an
omelette, or where to buy the best boots in London, things which have
no lustre taken from their setting, yet are positively of amazing
beauty within it. For it has come about, by the wise economy of
nature, that our modern spirit can almost dispense with language; the
commonest expressions do, since no expressions do; hence the most
ordinary conversation is often the most poetic, and the most poetic is
precisely that which cannot be written down. For which reasons we
leave a great blank here, which must be taken to indicate that the
space is filled to repletion. PAGE NUMBER
\end{quote}
The use of the space break, which is meant to signify everything that
passes between Orlando and Shel and more (“it is filled to repletion”)
functions by signifying nothing. According to critics like Katheryn
N. Benzel, this moment creates literal space for the reader to fill in
with her own interpretation of the scene. The text paradoxes, for the
reader to resolve, such as “the most ordinary conversation is often
the most poetic, and the most poetic is precisely that which cannot be
written down.” As a formal device, the space break, in Smith’s words,
“bemoan[s] the inadequacy of language” (Smith 68).

Here the narrator is saying that language doesn't execute -- it does
not enact. Its pithyness, just four words which begins with the
conditional "since" and the enactive "do," evoke a kind of
programmatic logic. And this programmatic logic hits its own
limitation, it can mean, it can even produce meaning, but it cannot
do. 

\section{works}
\label{sec:org3c5d414}
Buolamwini, Joy, and Timnit Gebru. “Gender Shades: Intersectional
 Accuracy Disparities in Commercial Gender Classification.”
 Proceedings of the 1st Conference on Fairness, Accountability and
 Transparency, PMLR, 2018, pp. 77–91.

Butler, Judith. "Performative acts and gender constitution: An essay
 in phenomenology and feminist theory." \emph{Feminist theory
 Reader}. Routledge, 2020. 353-361.

Hayles, N. Katherine. "Afterword: Learning to Read AI Texts" in "Again
 Theory: A Forum on Language, Meaning, and Intent in a Time of
 Stochastic Parrots," \emph{Critical Inquiry}. 30 June 2023.

Piper, Andrew \emph{\_akpiper}. "There is so much to say about the @nytimes
 \#BingChat transcript. That so many people are drawing on literary /
 film references to make sense of what is going on is telling." Feb
 16, 2023. \url{https://twitter.com/\_akpiper/status/1626239843905974274}

Turing, Alan. "Computing Machinery and Intelligence." \emph{Mind}
 59.236. 1950.

Underwood, Ted. "The Empirical Triumph of Theory" in "Again Theory: A
 Forum on Language, Meaning, and Intent in a Time of Stochastic
 Parrots," \emph{Critical Inquiry}. June 29, 2023.
\end{document}
