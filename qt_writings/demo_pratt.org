* goals:
- understanding of language models available to them and how to start
  using them on their own.
  - build toward skills for using them in programming, even if you
    don't have a background
- familiarity of underlying technical processes that power the tools,
  like word vectors.
- exposure to issues with training and producing LLMs, such as
  particularly the perpetuation of bias and discrimination and
  copyright issues.
- hands-on experimentation with tools in Python programming language.

* class
#+REVEAL_ROOT: http://cdn.jsdelivr.net/reveal.js/2.5.0/

# introduction to HF website, the tasks -> spaces path for navigating
# how to write a simple script for inference on colab
# introduction to pythonic concepts: abstraction and data types &
# structures

** Welcome!

:NOTES:

This workshop is a gentle introduction to using advanced AI tools, for beginners.

We will work on a popular platform for machine learning, the
HuggingFaceü§ó platform, which offers tools uploaded by a community of
AI developers. We will explore and play around with these tools, which
range over various tasks and topics. 

Then, we will shift to running these tools with coding, with the
Python programming language on Google Colab.

How many people have some background with Python?
- This workshop will be a gentle introduction to people who have never
  coded with Python before.

:end:

** Workshop goals:
- understanding of language models available to them and how to start
  using them on their own.
- familiarity of underlying technical processes that power the tools,
  like word vectors.
- exposure to issues with training and producing LLMs, such as
  particularly the perpetuation of bias and discrimination and
  copyright issues.
- critical reflection of methods for evaluating LLM performance.
- hands-on experimentation with tools in Python programming language.
- skills for importing and running models into your Python code.

** introduction to LLMs
# 20 min
# LLMs are prediction machines
# 1 key development:
# word embeddings, quantifying language

:notes:
Goal is to de-mystify how Large Language Models (a kind of AI
software) work under the hood.

Also to get a sense of the types of models that exist. There's a large
ecosystem of models, which can be overwhelming. 

All of this will help us in the next section, when we move to the HF
website. 
:end:

*** How does ChatGPT work?
How does it know what to respond when someone asks it a question?

:notes:
More specifically, how does it know what language to generate, what
words follow other words?
- by prediction.
- it learns by reading. Gains an understanding of language from
  processing massive amounts of text, deriving patterns.

One key development in history of AI: 
- quantifies the meanings of words; words in text become numerical
  representations (word embeddings)
:end:

*** Word Embeddings / Word Vectors

#+CAPTION: Image from "Word Embedding: Basics", by Hariom Gautam
#+attr_html: :width 500px
[[https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sAJdxEsDjsPMioHyzlN3_A.png]]

:NOTES:
How do we turn words into something that a computer can understand?

We use "word embeddings", "word vectors."
- technically: representations of language in vector space, graphical
  space.
- Each word is represented by a series of numbers,
  with each of those numbers representing it's relationship to another
  word. How closely they are related.
- show vector for "cat" vs "dog":

| word | tiger | cute | bones | wolf  |
|---+---+---+---+---|
| cat | .90 | .99 | .40 | .35 |
| dog | .35 | .99 | .85 | .90 |


Each word is expressed as a series of numbers. Each number a different dimension of the vector.

Except we don't have just x, y, or z. We have hundreds, thousands of
vectors.

Show image from Medium.
- More words here, along with their visualization in "vector space",
  which is just a graph.

Why do this to words? Why represent them as numbers?
- So we can do math!
- We can do linear algebra.
  - Cat and kitten are close to each other, that means they have
    similar meanings.
  - We can do cosine similarity, finding out what two vectors are
    similar to each other in shape/direction actually gives us a sense
    of their semantic similarity. Opens up a world of algebra,
    calculus, that we can do with language.

:END:

*** king - man + woman = queen
vector(‚ÄùKing‚Äù) - vector(‚ÄùMan‚Äù) + vector(‚ÄùWoman‚Äù) = vector("Queen")

:notes:
Famous formula:
- King - Man + Woman = Queen
  - we can do math using language! Or do language using math!?
- vector(‚ÄùKing‚Äù) - vector(‚ÄùMan‚Äù) + vector(‚ÄùWoman‚Äù) = vector("Queen")
- Notice for a moment all the assumptions being made about gender
  here.
  - That the difference between a king and a queen has to do with
    gender.
  - What exactly is being calculated when we subtract "man" and add
    "woman"?
    - Is it biological sex that's being substracted?
    - Is it gender conventions, femininity and masculinity? Kings are
      embody a masculine ideal, and queens a feminine one?
    - What qualities are being assumed to pertain to each gender and
      each
    role?
- Not a massive deal, but interesting, because this is the formula
  that introduced the power of word vectors to the world. So the
  assumptions it plays on must be deeply embedded across society.

Why am I saying all this about word vectors? 
- to de-mystify the tool.
  - these tools are not magic, they are not intuitive, possibly not
    even "intelligent", they can just do a lot of math.

See more:
- [[https://arxiv.org/abs/1301.3781][Word2Vec paper]], 2013.
- (and [[http://jalammar.github.io/illustrated-word2vec/][great explanation by Jay Alammar]])
:end:
** Huggingfaceü§ó platform
# 30 min
# how to navigate the models & datasets hubs
# introduction to inference, licensing, and data
# give some time to explore models and datasets

:notes:
Now let's move to the platform we will be using, HuggingFace. 
- HF is an AI research and development company based in Brooklyn, New
  York City.
- A platform for Machine Learning: compute & collaborative spaces for
  AI models, datasets, and more.
  - like a github for ML, if github had additional "hubs" for things
    besides just code (datasets, papers, apps).
  - also can run software directly on the website, "platform"
:end:     
    
*** "models hub"
# 10 minutes

:notes:
Start with the "models hub"

Here contains AI models created by the community/ HF users.
- a little overwhelming interface, I will explain it in a moment. 
- models are trained or fine-tuned, can then published to the "hub".
- navigation goes from left to right
  - on left side, there's tasks, like text classification. Also
    libraries, datasets, etc. Later we will choose one?
    What is the *difference between training and fine-tuning*?
     - training
       - the creation of a "base" model. It requires lots, LOTS of data,
	 gigabites of data, and compute power. It takes days, sometimes
	 longer.
     - fine-tuning:
       - taking a base model, which has already been trained (like BERT)
	 and training it further, with a much smaller dataset that is
	 focused on a specific topic.
       - customizing the model to work for a particular topic or kind of
	 data. 
	 - finBERT for sentiment analysis of financial data.
  - on right side, there's models. We are going to narrow down the
    models.

Filter results by "most downloaded", notice the difference:
- "most downloaded":
  - Wav2Vec - audio to vector, speech to text.
  - RoBERTa - one of the many permutations of BERT, you'll see. First
    model to put into practice the Transformer architecture.

Filter results by "most liked", you'll see things that may appear
familiar. 
  - [[https://huggingface.co/runwayml/stable-diffusion-v1-5][Stable Diffusion]] - image generator model.
  - models by Meta, aka Facebook.

Scroll down, and look for a model called "gpt-neo". Or search for it.
  - [[https://huggingface.co/EleutherAI/gpt-neo-125m][gpt-neo-125m]]
    - a model developed by EleutherAI, a non-profit research lab.
    - part of a larger family of models named "gpt-neo" with the size
      at the end.
      - search "gpt-neo" and you'll see the list. 
  - notice "*model size*". How big is it?
    - 125m parameters. That's how many inputs goes into
      inference. Includes things like word vectors, but also different
      kinds of inputs.
      - size is an indication of complexity. The larger the size, the
        more likely that the model will preform well.
  - notice the "*license*":
    - MIT license. Very permissive, part of the "Open Source"
      licenses.
      - the model is totally open to download and modify as you wish,
        even for commercial purposes.
    - practice running inference here for a few mintues.
      - anything that you notice about the results?
	- it's repetitive.
	  - the repetition problem is caused by the traits of our
            language itself.
	  - it generates words that have the highest likelihood. The
            words that have this likelihood tend to be the same ones,
            over and over again. 

Go back to most download, select [[https://huggingface.co/meta-llama/Llama-2-7b][Llama]],
- by Meta, aka Facebook.
- in terms of licensing, this is the most restrictive, by far.
  - Meta champions this model as "open source" but it is nothing like
    that. The license prevents you from making anything that can
    compete with them.
    - "open source" vs open access models: not everything open access
      is open source!
    - not sharing the model‚Äôs training data or the code used to train
      it unless you sign their agreement. 
:end:

*** "datasets hub"
# 10 minutes

:notes:
Besides models, ü§ó offers Datasets.
- these datasets are used to fine-tune (and also train and evaluate)
  models.
- "training data"

Before diving in, important to consider how these datasets were
created. 
- there are a lot of *ethical issues* with the ways that datasets for
  training are generated.
  - where the data comes from
  - and how they are cleanded (or not cleaned).

Where do we get most of the data used to train these models? 
  - scraped from the internet, most of them.

Search "bookcorpus"
- click on [[https://huggingface.co/datasets/bookcorpus][bookcorpus]] to open the dataset page
  - can see that it consists of sentences from books.
  - can also see that it's been used to train some of the most
    influential models out there. The Berts!
- Unfortunately, this dataset violates copyright.
  - scrapes all the free books from smashwords.com, even those that
    have copyright licenses to not reproduce information.
  - the authors obviously did not consent to their work being scraped.

This points to a major issue being explored in the courts today, the
issue of whether the training data violates copyright.
- The makers of these models argue that the use of training data falls
  under the "fair use" exception to copyright law.
  - Something can be used if it is for educational purposes.

Scraped from the internet also means bias
  - contains all the worst parts of the internet, too. All of the
    discrimination and violence.
  - you cannot automate the removal of bias and discrimination,
    because it can be situational, nuanced.
    - I might say something that is offensive, whereas if someone
      else, from different background or in a different context says
      the same thing, it's not offensive.
   - attempts to automate this have failed:
     - "List of Dirty Obscene..."
  - there's a race to get these products out there, so people aren't
    taking the time needed to adequately clean the data and make sure
    it's safe. That's just a fact.
    - RLHF - "reinforcement learning from human feedback"

Go back to main datasets page, select [[https://huggingface.co/datasets/truthful_qa][TruthfulQA]]
- this is a "benchmark" dataset, can also be used for training. 
  - it's meant to measure how well models perform.
  - in this case, to measure how truthful the output responses are to
    certain prompts.
- meant to measure "truth" of a model. If a model scores well on this
  benchmark, they are considered truthful.
  - these are the ways we are assessing our model's performance. 

Something to keep in mind! There's a lot of work to be done developing
datasets, for fine-tuning and benchmarking. 
:end:

*** think/pair/share activity
Something about why it is so difficult to automate bias?
(I will have given them clues to this throughout the first sections). 
** inference with python on colab: abstraction & data:
# 20 min
Now that you have a sense of how things work on the HF website, we are
going to practice running inference on Google Colab.

Our goal is to create a text generator, using Python code, taking the
following steps: 
- Will use the model, "[[https://huggingface.co/EleutherAI/gpt-neo-125m][gpt-neo-125m]]", and write code that imports this
  model into the colab coding space.
- Then we will write code to process an input text, and generate an
  output, a continuation.
- Finally, we will import a dataset from the library and practice
  running inference with it.
  
We'll talk about some programming concepts along the way, like
variables and data types. Accessing data from different types and
structures. 
- how programming languages abstract data through variables, learning
  how to read these layers of abstraction.
- how programming languages store data into structures, and how to
  access or manipulate that data. 

*** google colab, REPL, variables:
# 5 min
https://colab.research.google.com/

A cloud computing platform, where you can run code directly in the
browser.

Python can be difficult to install and configure, it's system
specific. Also distributions are large and take up space on your
laptop. Cloud computing takes away these issues.
- one particular plus is the colab offers computing power that is
  strong enough to handle these models.

Basic interface:
- cells to run the code, an "expression"

#+BEGIN_SRC python
1 + 1
# run code by pressing shift-return, or the play button. 

x = 5

y = 7

x + y

# all variables saved. 

# "interactive mode" - evaluate the expression, print result, back to
# prompt for more expressions.

#+end_src

*** import the models
# 2 min
on the toolbar, where it says RAM DISK, change the hardware accelator
to GPU.

Then go back to the models page.

Search for gpt-neo, select 125m. On the top right, click on "Use in
Transformers."

Copy that code, and paste it to your google colab cell.

#+BEGIN_SRC python
# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text-generation", model="EleutherAI/gpt-neo-125m")
#+END_SRC

Here we have a function, called ~pipeline()~, which takes parameters (a
fancy word for input).

The parameters specify the task and the model that we will be using.

We save the function to a variable called ~pipe~, which we will later
use to process our prompt. 

*** run inference
# 10 min

Now we are going to "run inference."

First, we will type up a prompt, and save it to a variable
~prompt~. Then we will pass that prompt to the ~pipe~ variable that we
created before, saving the output to a new variable, called ~output~. 

#+begin_src python
  prompt = "Hello, my name is Filipa and"

  # we want the length to be 50 characters at the most 
  output = pipe(prompt, max_length = 50)
#+end_src

Here we see the levels of abstraction at play. Saving the pipeline
function to a new variable, then the prompt text to a variable, and
passing that prompt into the pipe.

Now let's look at the response, and inspect the data structure
contained within it, which is a ~list~.

~list~ is a collection of objects, or bits of information. So our
output is saved as this collection type of object.

#+begin_src python

output
# [{'generated_text': "Hello, my name is Filipa and I'm a newbie in
# the world of web development."}]

type(output)
# list

#+end_src

What if we wanted to extract just the output text, not the rest of the
data, how would we go about it?

We are going to examine this list to see what else is contained
inside. For that we will use "indexing."

Indexing is picking out object by their position within another
object, like a list (though it also works for strings). The first item
is zero, the second item is 1, and so on.

Does anybody know what we start with zero? (Because it is based on
offsets. Think like a computer. The first item is the starting place,
we don't have to move anywhere to access it. But the second item, we
have to move one place to the right, so it's 1). 

#+begin_src python
  name = Filipa
  name[0]
  # F
  name[1]
  # i

  output[0]
  # {'generated_text': "Hello, my name is Filipa and I'm a newbie in the
  # world of web development."}

  type(output[0])
  # dict

#+end_src

Now we are getting closer, we got rid of the brackets. Inside this
list, we actually have a new data type, called a ~dict~. This stands
for data structured into key:value pairs.

Let's look at an example:

#+begin_src python 

  # key, value pairs

  filipa = {
      'first_name': 'filipa'
      'last_name': 'calado',
      'job': 'library',
      'age': '34',
      'degree': 'literature'
  }


  type(filipa)
  # dict

#+end_src

To get items from a dict, you use a different method, accessing them
by their keys.

#+begin_src python
  filipa['first_name']
  # filipa

  filipa['degree']
  # literature

#+end_src

So, we can combine what we know about list indexing and accessing
items in a dict by keys to pull out just the response text

#+begin_src python

  output[0]['generated_text']

  # then we can save it to a variable!

  text = output[0]['generated_text']

#+end_src

