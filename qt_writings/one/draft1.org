* one

*** revision notes
**** chapter summary 
This chapter proposes a reading methodology that leverages the
critic's relationship to the text to open possibilities for
interpretation and connections to the textual material. It explores
the ways that reading practices across two different fields (digital
humanities and queer theory) intertwine, and how this creates a new
method for reading queer narratives in digital contexts. The burden of
the chapter is to describe how queerness and data are mediated, and
can/should be engaged critically as constructions, formalizations,
that draw attention to their own mediation.

*We examine an impulse in DH which we call the "fantasy of falsifiable
criticism."* This fantasy is where queerness and data cannot be
captured in their raw forms.

Looking at the example of DH from the position of queerness we see
more clearly the necessity of opacity, formalization, and abstraction
as reading methods, and the real danger of reproducibility and
totalization, byproducts of paranoia. As a solution, We propose
novelty (the performative). We see this in /voyant-tools/ with
/Orlando: A Biography/.

**** revision feedback from diss 

Underwood & Da
- While before the emphasis was on data organization, now it is on the
  choice/selection of data. This is okay, but the differences between
  the two need to be acknowledged. 
- Thread the notion on collapsing assumptions about gender throughout
  the entire section, not just the Underwood critique. This needs to
  come to the fore of the argument. 
- Consolidate the quality that I'm trying to emphasize with my
  Underwood critique: is it standardization, ease of use, simplicity,
  or reproducibility??
- Address how Da's misunderstanding of Topic Modelling has been
  answered by Ben Schmidt in Critical Inquiry---not that this matters,
  because it nonetheless reveals a perspective on critical methods
  which values the reproducible. 
- Explain better reproducibility. 


** on reproducible criticism
*** Underwood et al

Major developments in technology also perpetuate racial
assumptions. Moving from networking technologies to software
development, Tara McPherson explores the parallels between the
Operating Systems and race relations, to show how the development of
computer software betrays hegemonic assumptions about whiteness and
elisions of difference.[fn:1] She focuses on the key moment of 1960s
United States, when Operating Systems, which is the foundational
software that supports a computer's programs and basic functioning,
developed alongside civil rights discourses. Her research focuses on
how "the organization of information and capital" in OS development
resonates in the struggles for racial justice: "Many of these shifts
were enacted in the name of liberalism, aimed at distancing the overt
racism of the past even as they contained and cordoned off progressive
radicalism" (30). McPherson deconstructs the UNIX operating system
which includes a hierarchical file system, a command line interpreter
(the Terminal on Mac or Command Prompt on Windows), and a variety of
software programs that are designed to work in tandem. McPherson
points out that UNIX-based Operating Systems (like Mac and Linux) are
distinguished by the ways that they partition and simplify complex
processes into discrete components, similar to the ways that identity
politics cordones off parts of the (social and technological) system
into distinct units. While this cordoning was productive for the
promotion of civil rights, it also, according to McPherson, "curtailed
and short-circuited more radical forms of political praxis, reducing
struggle to fairly discrete parameters" (30).

Crystallizing the intersection between Operating Systems and race
relations, McPherson asserts that "Certain modes of racial visibility
and knowing coincide or dovetail with specific ways of organizing
data" (24). McPherson emphasizes the "rules" of UNIX philosophy, which
lay out how UNIX's development prioritized the organization and
simplification of data processing:
#+BEGIN_QUOTE
Rule of Simplicity: Design for simplicity; add complexity only where
you must. Rule of Parsimony: Write a big program only when it is clear
by demonstration that nothing else will do. Rule of Transparency:
Design for visibility to make inspection and debugging easier... Rule
of Representation: Fold knowledge into data so program logic can be
stupid and robust. 26
#+END_QUOTE
The rules of "Simplicity" and "Parsimony" ensure that programs will be
composed of small, interlocking parts that can be easily updated and
transported to newer versions. The rule of "Transparency" flattens
nuance and ambiguity, making program components as legible as
possible. The rule of "Representation," particularly the suggestion to
"Fold knowledge into data" reduces the complexity of raw data, so that
it can be easily input into multiple processes. According to
McPherson, all of these rules work together to shore up the central
design theory of "modularity,"[fn:2] which stipulates that components
are self-contained and interoperable, so they can be independently
created, modified, and replaced without affecting the whole system.

The role of control in creating the internet and the emphasis on data
reduction in developing operating stystems leave their legacies on
21st century digital technology, where race becomes collapsed into
data. Echoing McPherson, Ruha Benjamin asserts that technology
reproduces social inequities under the guise of objectivity and
progressivism.[fn:3] Turning to technology, Benjamin explores how
innovations in Artificial Intelligence and algorithmic computing
extend racist paradigms into ever new tools, particularly in data
gathering and surveillance. The creators of these new technologies
mark, track, and quantify blackness, for example, in databases for
healthcare or financial services that associate "black names" with
criminality (Benjamin 5). With each update, technology is continually
promoted as efficient and progressive in a way that masks how it
exploits data about its subjects. Benjamin explains, "we are told that
how tech sees “difference” is a more objective reflection of reality
than if a mere human produced the same results... bias enters through
the backdoor of design optimization in which the humans who create the
algorithms are hidden from view" (5-6). As she points out, "the road
to inequity is paved with technical fixes” (7). Like the creators of
UNIX, the creators of such tools and algorithms operate under
assumptions of white universality that inevitably marks blackness as
"other."

*** Underwood & Da on reproducibility

Let us now turn to computational methods, seeing how they bear out
some of the legacies from the above technological
histories. Practitioners of "distant reading," a critical method at
the intersection of Literary Studies and Data Science, use
quantitative analysis to study works of literature. This process
involves deploying computer programs to clean, categorize, and count
elements in textual data, and is often followed by interpretive
analysis, where the critic engages the results of quantification from
a humanities lense. More often than not, distant reading is combined
with close reading methods, as crtics will use the results of
quantitative analysis to identify key moments from the text that merit
closer attention.[fn:4]

According to its practitioners, distant reading is most useful for the
ways it allows connections to emerge among vast amounts of textual
data. Critics who do this work often emphasize the problem of literary
scale and human attention, because distant reading allows them to
handle the thousands of books in literary history without actually
reading these texts. One prominent practitioner of Computational
Literary Studies (CLS), Ted Underwood,[fn:5] harnesses the power of
quantification and machine learning to glimpse what he calls the
"distant horizon" of literary trends across centuries. His argument
convincingly begins with the observation that human capacities of
sight, attention, and memory preclude them from grasping the larger
patterns of literary history across time. Distant reading, where
"distance" means abstraction, or the simplification of textual data
into computable objects such as publication dates and genres, allows
critics to see connections amid the swarm of overflowing information.

Among distant reading practitioners, Underwood's approach is unique in
that he models the ways that human assumptions can affect the results
of analysis. Underwood is careful to point out the subjective nature
of his method, which he calls "perspectival modelling," by turning it
into an object of study. He uses machine learning, or programs
"trained" by certain data sets, to create models that can then make
predictions on other datasets. He explains that, "Since learning
algorithms rely on examples rather than fixed definitions, they can be
used to model the tacit assumptions shared by particular communities
of production or reception" ("Machine Learning and Human Perspective"
93). One of his projects examines gender roles in novels from the
18th century to the 21st century by using a machine-learning model to
"guess" the sex of a fictional character based on the words associated
with that character. Underwood explains how the test is configured:
#+BEGIN_QUOTE 
We represent each character by the adjectives that modify them, verbs
they govern and so on--excluding only words that explicitly name a
gendered role like /boyhood/ or /wife/. Then, we present characters,
labeled with grammatical gender, to a learning algorithm. The
algorithm will learn what it means to be 'masculine' or 'feminine'
purely by observing what men and women actually do in stories. The
model produced by the algorithm can make predictions about other
characters, previously unseen. /Distant Horizons/ 115
#+END_QUOTE
In simplest terms, the program studies some given adjectives
associated with a male or female character in order to make
predictions about other characters' genders. Inevitably, the resulting
output is always determined by this initial input. Underwood carefully
asserts that these models reveal, not the truth of literary histroy,
but the approaches and choices made by those who create the models:
"Machine learning algorithms are actually bad at being objective and
rather good at absorbing human perspectives implicit in the evidence
used to train them" ("Machine Learning and Human Perspective"
92). This particular model reveals that that, over time, gender roles
in novels become more flexible while the actual number of female
characters declines (/Distant Horizons/ 114). The graph shows a steady
overlapping of words traditionally associated with women, such as
"heart," with words typically assoicated with men, like "passion,"
toward the middle of the 20th century. One of the many explanations
for this result, Underwood reasons, is that the practice of writing
became more commonly pursued as a male occupation in the middle of the
20th century than it was previously (/Distant Horizons/ 137). This
fact, coupled with the tendency of men to write more about men than
women, suggests why less women writing would led to a decline in
female characters. This explains how Underwood's seemingly paradoxical conclusion, that gender roles become more flexible while the actual prevalence of women dissapates from fiction, might be possible.

However, the results of Underwood's "perspectival modeling" can only
be as good as the questions he asks. From a critical gender
perspective, Underwood's approach imposes the very structure that he
is attempting to deconstruct. In other project, he where he similarly
measures the "transformations" of gender across time periods, he
explains that simplification is necessary ("Machine Learnig and Human
Perspective" 93):
#+BEGIN_QUOTE
I recognize that gender theorists will be frustrated by the binary
structure of the diagram. To be sure, this binary has folded back on
itself, in order to acknowledge that social systems look different
from different positions in the system. But the diagram does still
reduce the complex reality of gender identification to two public
roles: men and women. I needed a simple picture, frankly, in order to
explain how a quantitative model can be said to represent a
perspective. "Machine Learning" 98
#+END_QUOTE
Underwood admits that he needs a "simple" model in order to bring into
relation the dynamics of gender (See Fig. 2).[fn:6] However, he
underestimates the extent to which his initial assumptions determine
the final result. Although he considers the possibility that he finds
a structural tension between gender "because [he] explores gender, for
the most part, as a binary opposition" (/Distant Horizons 140), he
neglects to consider how the collapsing of gender into a single graph
perpetuates the structural categories of male/female in a way that is
neglects the assumptions behind such a category.[fn:7] Moreover, the
issue is not just with the assumptions at the outset which reproduces
the result, but with the guiding question of the entire project, which
is not about deconstructing gender, but about reifying it. To begin
with, why should humanists seek to automate the conscription of gender
norms within these terms? Asking a machine to replicate the
conscription of gender for the purpose of seeing how male and female
roles in novels change over time only creates a model of gender that
is "simple" enough to be computed by the system. How does simplifying
the concept of gender contribute to our study of it? The results of
using the machine can only be as good as the questions we ask.

[[./img/Underwood.png]]

Critiquing scholars like Underwood, Nan Z. Da argues that quantitative
methods are ill-suited for literary criticism. She accusses Underwood
and other distant reading practitioners for trading "speed for
accuracy, and coverage for nuance" (620). Of her many gripes with
quantitative methods, which include "technical problems, logical
fallacies," and a "fundamental mismatch betwen the statistical tools
that are used and the objects to which they are applied" (601), she
emphasizes the lack of reproducible results, the idea that one
researcher's process can be reproduced by another with identical
output, which is essential to statistical methodologies. She
demonstrates with an experiment of Topic Modelling, which is the
processing of large texts in order to generate a number of "topics"
within the corpus. Researchers often use Topic Modelling as a way of
speed-reading a massive corpus to get a sense of what it is about
without having to actually look at the text. Da attempts to verify the
results of a Topic Modelling experiment by replicating the process on
her own machine, a replication that fails. She concludes that, "if the
method were effective, someone with comparable training should be able
to use the same parameters to get basically the same results"
(628-629).[fn:8] For Da, reproducibility of method is a benchmark for
reviewing and assessing the efficacy of quantification.

Despite their vastly different committments, scholars like Underwood
align with Da on the value that they place on reproducibility, which
is an ultimately conservative investment. Underwood demonstrates how
the critic reproduces their assumptions in the questions and data used
at the outset in a way that structures the final result. Da's emphasis
on the reproducible suggests that, to be useful, quantitative literary
criticism ought to resemble something more like statistical analysis:
if the method can be verified, can be copied and reproduced, then the
interpretive conditions might be universalized. 

*** Drucker's skewing the graphs

Underwood and Da overlook the way that quantification can be used to
disrupt assumptions or reveal the constructed nature of data. In
contrast to Underwood and Da, Johanna Drucker is careful to dispell
the illusion of "raw data," which comes already reduced to fit
whatever parameters required by analysis. Because data always
undergoes a transformation in order to be quantified, its complexity
is always reduced. As a result, Drucker argues, quantification
techniques such as visualizations in graphs and charts inevitably
misrepresent the data they are meant to convey. To illustrate this
process, Drucker presents a chart displaying the amount of books
published over several years. The chart appears to convey production
during this specific time period, but Drucker explains that
publication date is an arbitrary metric for capturing
production.[fn:9] She brings to the surface all the assumptions made
in such a metric, for example, the limitations of "novel" as a genre
and the connotations behind "published," which suggests date of
appearance, but has no indication of composition, editing, review,
distribution. Each piece of data carries with it the result of many
interpretive decisions, that carry with them varying degrees of
opacity, which are all necessary in order to present complex concepts
like book production as a bar on a chart. Drucker explains: "the
graphical presentation of supposedly self-evident
information... conceals these complexities, and the interpretative
factors that bring the numerics into being, under a guise of graphical
legibility" (Drucker par. 23).

To resist the reductions of "data," a term that deceptively connotes
that which is "given," Drucker proposes thinking of data as "capta,"
which suggests that which is taken. Drucker's "capta" is deliberately
creative, turning graphical expressions into expressive metrics:
components used for measurement, like lines or bars on a graph, break,
blur, or bleed into one another. Objects are not discrete entities,
but interact with the other objects in the visualization. For example,
in a bar graph of book publications by year, she warps the graphical
metrics, making some of them fuzzy, wider, shorter, in an attempt to
show that publication as a metric elides other information such as
composition, editing, purchasing, etc.

[[./img/Drucker.png]]

Emphasizing "capta" is a way of figuring elements that have been
reduced, resolved, or ignored in traditional quantitative
analysis. Drucker makes evident what is overlooked or assumed when
dealing with complex subjects by muddling (rather than simplifying)
the relationship between elements.

[The next step, which I want to take here, is to show how paying
attention to the assumptions (deconstructing) is a return to
embodiment. Allows us back into the concept of touching--mirrored in
the queer form section]

**** TODO add Mandell on gender as social construction
 

** intersection btw queer & digit
*** Butler on contact, embodiment


** Performativity/Movement

*** The value of alterneity over reproduction: performance
In the section on reproducibility, I discuss how Underwood's analysis
on gender differences reproduces his assumptions about gender dynamics
as oppositional, which he readily admits: "this chapter has discovered
stable 'structural positions' only because it explores gender, for the
most part, as a binary opposition" (/Distant Horizons/ 140). The
question then becomes, how can we move beyond reproducing assumptions
in our analysis? The answer is to shift the objective of analysis from
the the reproducible to the alternative. The first value that this
reading method proposes is that of /performance/. This value points to
the active qualities of critical analysis, emphasizing materiality and
sensitivity, movement and discovery. When reading is performative, the
process is more important than the product. To demonstrate this value
in practice, I turn to the work of Katherine Bode and Tanya Clement,
both of whom have deep investments with traditions of textual
scholarship, particularly the scholarship of Jerome McGann, that has
influenced early experiments with digital humanities in English
departments. Although their approaches vary in their specific topics,
methods, and results, they are connected in an investment for, in the
words of McGann, "imagining what we don't know" (82).

*** Bode's materiality,  critque of Underwood
--> bode emphasizes how inquiry implicates the researcher, who
generates at the same time that she analyzes data. Instead of looking
at what is being reproduced, look at how human engagement has
entangled with and created the object of analysis.

Begin the QLS work by examining gaps and biases: “quantitative
literary studies should begin by trying, as much as possible, to
consider the nature of ontological gaps and epistemological biases in
its evidence” (Bode "Model Away Bias" 97).

Katherine Bode offers a method that builds off the humanistic
approaches in textual scholarship and bibliography. Her work explores
the boundary between the humanities and social sciences in order to
reframe analysis as performative. Bode argues against the trend of
representationalism, "the idea that a knowing human agent symbolically
expresses – or represents – some thing-in-the-world (that thing is
unchanged by that expression, and that expression is more available or
apprehensible to the subject than the thing itself)--in digital
literary studies ("Data Beyond Representation" par. 2). Pushing
against this assumption of representation in computational modelling,
she explains that "entities don’t pre-exist engagements but are
generated in an ongoing or emergent way, by those intra-actions"
("Data Beyond Represenation" par. 2). This is not to say that one can
refrain from implication with the object of study. Rather, a
performative approach assumes such implication to be the starting
point of analysis: "all inquiries create boundaries (or cuts) in a
complex reality that can be organised in other ways; and all such
boundary-making practices are inevitably biased at the same time as
they are a condition of inquiry" (Data Beyond Representation
par. 16). The point, for Bode, is to examine "how... we inscribe the
boundaries we often presume to represent" ("Data Beyond Representation
par 11.)

Her current project, /Reading at the Interface/, examines the ways
that Australian literature has been characterized by various
"paratexts," or "writings about literature." The project explores
alternative understandings of Australian literature across various
platforms, including academic journals, newspapers, /Goodreads/, and
/Librarything/.


"In mining /Goodreads/, for instance, using a list of works defined by
an academic bibliography, I’m not interested in representing
discussion of “Australian literature” on Goodreads so much as in
materialising that platform in ways that cannot be separated from my
categories of analysis" (Data Beyond Representation par. 19).





For Bode, what statisticians value as “representativeness” or
“reproducibility” isn’t as important (within a humanities context) as
the materiality of the apparatus. Rather than attempt to secure a
factual or objective status of the data, we should double down on the
idiosyncracies of our tools. Accordingly, Bode suggests that we
approach literary databases in performative terms, taking a
self-conscious appraisal of the tools of analysis, as "effects of
material-semiotic engagements" ("Data Beyond Representation" 15).
- "at present, discussion of “representativeness” and
  “reproducibility” are bound up together, with the implication that
  if we can represent something accurately enough the results of
  analysis will be reproducible. Foregrounding the apparatus, by
  contrast, recognises that our knowledge making practices, as Karen
  Barad puts it, “contribute to, and are part of, the phenomena we
  describe”" (Bode "Data Beyond Representation, par. 26).

"I’m exploring what it might mean to conceive of literary databases as
apparatuses, in the sense the term is used in various scientific
disciplines, particularly physics. There, an apparatus is a specific
material configuration, including of physicists, wherein certain
properties become determinate, while others are excluded. One can’t
measure light as a particle and a wave using the same apparatus; but
that doesn’t mean that light is not one thing when it is measured as
the other. Although it must be said that the phenomena explored in
digital literary studies are much more diverse than those for which
apparatuses in physics are developed, I wonder if shifting to a
conception of measurements as effects of particular material
arrangements might help us to reframe some key debates in our field."
(Bode "Data Beyond Representation, par. 24).

*** Critique of Underwood's "sensitivity"---a focus on attention
Underwood overlooks the ways that quantitative literary analysis, or
distant reading, enables "sensitive" readings of textual
material. According to him, such methods are less useful for studying
a single text in depth and more useful for taking a long view of
larger corpora. He sets up an opposition between computer and human
reading: "Computational analysis of a text is more flexible than it
used to be, but it is still quite crude compared to human reading; it
helps mainly with questions where evidence is simply too big to fit in
a single reader's memory" (xxi). Underwood is right to point out that
a computer cannot draw inferences like a human can. However, his
emphasis on the role of memory opens up the ways that computers can
enhance human reading of smaller texts. What the computer properly
does is arrange a set of data--of any size--for human
consumption. This involves processing datasets into new formats that
can than be scrutinzed by a human reader. Underwood's goal, which is
"to find a perspective that makes the descriptions preferred by
eighteenth-, nineteenth-, and twentieth-century scholars all congruent
with each other," shows one potential objective for such reading
(/Distant Horizons/ 32). But there is more than one objective for
using quantitative methods regarding memory, and that is by
approaching memory, specifically human attention spans, as a drive,
rather than a hindrance. The computer can arrange text in a way that
harnesses the attention span of the reader.

*** Tanya Clement & Jerome McGann: performance --> discovery
Tanya Clement and Jerome McGann have written on how electronic
environments facilitate active experiences with text. Their analyses
draw attention to the ways that the reading process engages with the
situatedness of time, space, and textual objects that are entangled
within a complicated network of production and reception. Such a
reading process yields unexpected and alternative interpretive
possibilities. Clement's textual scholarship works with sound to
develop an hermeneutics that incorporates praxis, visualization,
embodiment, and play, toward a theory of performantive criticism. She
often questions how working with audio allows us to reconsider the
ways we approach electronic text. In one project, she explores how
visualizations of audio information can influence analysis. She puts
forth a theory of “play” in which the critic "performs" the work, much
like the way that musicians interpret a musical score. Clement makes
the analogy between musical scores and quantitative visualizations to
emphasize how both "create another level of abstraction with which the
interpreter engages" ("Distant Listening par. 7). These visualizations
use the audio analysis tool ProseVis to create dynamic spaces for the
reader to interact with a digitzed object. Using ProseVis, the reader
can navigate through the visualizations and manipulate the metrics for
analysis, in this case, the prosodic elements of Gertrude Stein's
poetry. Clement draws out the comparison between musical scores and
visualizations to emphasize the performative qualities of
analysis. She begins by describing the qualities of a musical score:

#+BEGIN_QUOTE
[I]t is read, but it is also meant to be played, to be spatialized in
time and embodied by voices (or instruments) within a certain physical
and hermeneutical context. I am arguing the same is true of
computational visualizations of text. One 'reads' a visualization, but
to 'play' the visualisation is to engage the spatialized
interpretation of that visualisation as an embodied reader in a
situated context within a specific hermeneutical framework. "Distant
Listening" par. 10
#+END_QUOTE

Like a musical score, which "point[s] toward many possible
interpretive 'results' or readings," visualizaions can provide a
starting ground for different pathways of analysis ("Distant
Listening" par. 12). Clement's scholarship on audio visualization
magnifies the importance of performance as an element in analysis. 

McGann's work on textual scholarshop similarly draws attention to the
effect of performance on interpretation, or performance /as/
interpretation, according to McGann. Along with Lisa Samuels, McGann
coins the concept of "deformance," which describes any activity that
distorts, disorders, or re-assembles literary texts to discover new
insights about its formal significance and meaning. They offer the
example of reading a poem backward, where “the critical and
interpretive question is not 'What does the poem mean?' but 'How do we
release or expose this poem’s possibilities for meaning?'"
(108). Deformance works by estranging the reader from her familiarity
of the text, and relies on the the volitality of meaning of particular
words that depend upon a multitude of factors, from antecedent
readings and pathways through that text, to the significance of
immanent elements such as typography and blank spaces, all of which
the reader can only process a limited amount. Digital tools might work
alongside this volatile potential for meaning, what McGann calls the
text's "quantum poetics." He explains that, “Aesthetic space is
organized like quantum space, where the ‘identity’ of the elements
making up the space are perceived to shift and change, even reverse
themselves, when measures of attention move across discrete quantum
levels” (183). McGann speculates that engaging with texts on a
computer could be as intimate a process as engaging with them on
paper, with the additional ability of manipulating and transforming
them in virtually infinite ways. Ideally, the tool should work as a
“prosthetic extension of that demand for critical reflection,” with
which the reader is able to feel her way through the text (18).

Clement and McGann's approaches facilitate a reading method that uses
computational tools in the aid of discovery. Human attention spans,
rather than represent the hurdle for computational methods to
overcome, offer an opportunity for re-imagining analysis as a process
deforming what we pay attention to. The unique affordance of digital
environments, according to McGann and Clement, is that they allow for
numerable interventions upon the textual object. The emphasis shifts
from viewing text as something stable and self-evident to something
dynamic and subject to different readings. As Clement explains: “A
model of textuality that represents text as a spatial and temporal
phenomenon might allow for interactions and representations in a
digital environment that, rather than insisting on fixity, foreground
principles of emergence” ("Rationale" 34).



** Misc
*** Altschuler and Weimar on reproducibility

--> reproducing something perfectly overlooks the ways that all
digital objects are unique, differentiated. Theory of textual
criticism which shows how ther are more interesting things to do then
create a digital "copy texte". 

This notion extends to digital humanist practitioners. 

they call to overturn the "unproblematic translatability of
information between the senses" while maintaining that reproduction is
the highest value. They argue to "texture the humanities", pointing
out that much of DH prioritizes the visual over other senses --
"privilege sight as the sense through which knowledge is accessible"
(74). Rightly so, they argue, “The textured DH we call for here
acknowledges that we cannot study knowledge only abstractly, apart
from the senses, and that we cannot study literature, art, and history
without including the history of embodied experiences” (74-75).
- “Touch This Page! uses 3-D printed facsimiles of raised-letter text
  to inspire reflection on the assumptions most people make about
  which senses are involved in reading” (82).

But they elide the one interesting trajectory when they place
reproduction over remediation/deformance. They state their aims: “to
expand the sensory accessibility of archives for all users and to do
so through the digital reproduction---rather than the translation---of
tactile knowledge” (76). Case example of the perfect reproduction:
- A scenario where “users... can download a visual copy with
descriptive data, engage with the text in virtual reality, and create
their own textured facsimile. This technology once more makes possible
the tactile reading experiences for which this volume was designed and
promises library patrons a richer engagement with touch than most
archives can currently provide---even in person (85-86). 

The use case scenario makes the assumption that a reproduction is the
ideal form of textuality, despite their asserted aims for "diversity
of embodied experiences":
- “we must avoid tilting after the fiction of some ideal digital
  surrogate---like a virtual reality system that would flawlessly
  mimic original objects---lest we become digital Pierre Menards,
  expending extensive energy to improve our reproductions to discover,
  at last, that only the original perfects represents itself… Instead,
  we envision in our tactile futures multiple strategies that could
  not only open up access to varied experiences---past and
  present---but also diversity the ways embodied experiences structure
  our digital worlds” (86).
- in order to open up “multiple strategies” and diversity embodied
  experiences, we need a theory of text that is capacious enough to
  accept variation and transmediation.
- This argument overlooks deformance is a solution: the ways that
  creating new texts, paratexts, creates new objects of knowledge. It
  overlooks the performative, ala McGann, Clement.

In this view, digital becomes a means of optimization, efficiency,
total knowledge and understanding.

* commands
c-c c-x f => create a new footnote
c-u c-c c-x f then select s => renumber footnotes

block quotes: #+BEGIN_QUOTE & #+END_QUOTE

* Footnotes

[fn:1] Tara McPherson’s “U.S. Operating Systems at Mid-Century: The
Intertwining of Race and UNIX," Race After The Internet, ed. Lisa
Nakamura and Peter A. Chow-White. Routledge, 2012.

[fn:2] Potentially revise and deepen this section by linking to Barad
& Haraway on situated knowledges and feminist science: Being modular
in itself isn't bad, as long as you are aware of the ways that
modularity creates limitations/reductions of data. Modularity needs a
critical awareness of its own tools.

[fn:3] Her work also extends Michelle Alexander's ideas from /The New
Jim Crow/ (2010), which argues that modern society perpetuates racist
violence and segregation by criminalizing race through the war on
drugs and mass incarceration.

[fn:4] Andrew Piper's methodology, which he calls "bifocal" reading,
demonstrates how distant and close reading are used together, with
distant reading providing the context or framework that guides close
reading"“We are no longer using our own judgments as benchmarks... but
explicitly constructing the context through which something is seen as
significant (and the means through which significance is
assessed).... It interweaves subjectivity with objects” (Piper,
Andrew. Enumerations: Data and Literary Study, 2018, 17).

[fn:5] Underwood, Ted. /Distant Horizons/, 2019.; Underwood,
Ted. “Machine Learning and Human Perspective.” PMLA, Vol. 35 No. 1,
January 2020, pp. 92-109.

[fn:6] He measures the "gendering of words used in characterization"
("Machine Learning and Human Perspective" 95), that is, gender
portrayed in novels by women and in novels by men. The verticle axis
visualizes the representation of words by women, and the horizontal by
men, with positive numbers signifying overrepresentation of these
terms. So terms on the top right are words that are used often by men
and women writers, and terms in the upper left and lower right are
ones used most often by women and men, respectively.

[fn:7] Add a quote here from Laura Mandell on F/M categories?

[fn:8] Da's emphasis on the “reproducible” in CLS extends Franco
Moretti's originating call for a “falsifiable criticism”: both
advocate for a methodology that is as reliable and verifiable as the
social sciences. According to Moretti: “Testing” literary
interpretations be the same process as in scientific disciplines --
demanding that interpretations are “coherent, univocal, and complete,”
and are tested against “data” that appears to contradict it (/Signs/
21). (another quote: “The day criticism gives up its battle cry ‘it is
possible to interpret this element in the following way,’ to replace
it with the much more prosaic, ‘the following interpretation is
impossible for such and such a reason,’ it will have taken a huge step
forward on the road of methodological solidity” (/Signs/ 22).)

[fn:9] Drucker implicitly refers to the first chapter from Franco
Moretti's /Graphs, Maps, Trees/ (2007), throughout which Moretti
graphs novels by their publication date between 1700 and 2000 and
draws conclusions about the relationship between genre and generations
of readers.

