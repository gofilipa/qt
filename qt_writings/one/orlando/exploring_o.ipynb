{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "my_url = \"http://gutenberg.net.au/ebooks02/0200331.txt\"\n",
    "file = urlopen(my_url)\n",
    "raw = file.read()\n",
    "orlando = raw.decode()\n",
    "o_tokens = nltk.word_tokenize(orlando)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_text = o_tokens[872:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orl = nltk.Text(o_text)\n",
    "orl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning the text (caps, punct, stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orl[:41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercasing and removing punct\n",
    "lower_no_punct = []\n",
    "for word in orl:\n",
    "    if word.isalpha():\n",
    "        lower_no_punct.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "# removing stops\n",
    "no_stops = [word for word in lower_no_punct if word not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stops[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "wordnet_lemmatizer.lemmatize(\"slicings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = []\n",
    "for word in no_stops:\n",
    "    word_lem = wordnet_lemmatizer.lemmatize(word)\n",
    "    clean_text.append(word_lem)\n",
    "    \n",
    "clean_text[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orl = clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# similar words, first level\n",
    "\n",
    "to compute distinctive similarity, we:\n",
    "- make a list of words similar to \"man\" or \"woman\" using Text.similar()\n",
    "- find words similar to those words\n",
    "- filter out the words that are shared among the two lists\n",
    "- repeat as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make these lists of distictive similar words automatically, \n",
    "# use ContextIndex class\n",
    "idx = nltk.text.ContextIndex(orl)\n",
    "# put words similar to woman in a list\n",
    "woman_sim_1 = idx.similar_words(\"woman\")\n",
    "woman_sim_1_str = \" \".join(woman_sim_1)\n",
    "woman_sim_1_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ebef6b962512>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# put words similar to man in a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mman_sim_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"man\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mman_sim_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mman_sim_1_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mman_sim_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mman_sim_1_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'idx' is not defined"
     ]
    }
   ],
   "source": [
    "# put words similar to man in a list\n",
    "man_sim_1 = idx.similar_words(\"man\")\n",
    "man_sim_1\n",
    "man_sim_1_str = \" \".join(man_sim_1)\n",
    "man_sim_1_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops through woman similar and filters out words shared with man similar\n",
    "woman_disc_1 = []\n",
    "for word in woman_sim_1:\n",
    "    if word not in man_sim_1:\n",
    "        woman_disc_1.append(word)\n",
    "woman_disc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops through man similar and filters out words shared with woman similar\n",
    "man_disc_1 = []\n",
    "for word in man_sim_1:\n",
    "    if word not in woman_sim_1:\n",
    "        man_disc_1.append(word)\n",
    "man_disc_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# similar words, second level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop that computes similar words for each word from woman_sim_1\n",
    "woman_sim_2_nested = []\n",
    "for word in woman_sim_1:\n",
    "    woman_sim_2_nested.append(idx.similar_words(word)) # this returns nested lists\n",
    "\n",
    "# list comprehension that flattens nested list\n",
    "woman_sim_2 = [inner\n",
    "    for outer in woman_sim_2_nested\n",
    "        for inner in outer]\n",
    "\n",
    "woman_sim_2_str =\" \".join(set(woman_sim_2))\n",
    "woman_sim_2_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(woman_sim_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a loop to find similar words to man_sim_1\n",
    "man_sim_2_nested = []\n",
    "for word in man_sim_1:\n",
    "    man_sim_2_nested.append(idx.similar_words(word))\n",
    "man_sim_2_nested\n",
    "\n",
    "# collapsing the nested list\n",
    "man_sim_2 = [inner\n",
    "    for outer in man_sim_2_nested\n",
    "        for inner in outer]\n",
    "\n",
    "man_sim_2_str =\" \".join(set(man_sim_2))\n",
    "man_sim_2_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops through woman similar and filters out words shared with man similar\n",
    "woman_disc_2 = []\n",
    "for word in woman_sim_2:\n",
    "    if word not in man_sim_2:\n",
    "        woman_disc_2.append(word)\n",
    "woman_disc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(woman_disc_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops through woman similar and filters out words shared with man similar\n",
    "man_disc_2 = []\n",
    "for word in man_sim_2:\n",
    "    if word not in woman_sim_2:\n",
    "        man_disc_2.append(word)\n",
    "man_disc_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd level words similar to woman_disc_2\n",
    "woman_sim_3_nested = []\n",
    "for word in woman_sim_2:\n",
    "    woman_sim_3_nested.append(idx.similar_words(word)) # this returns nested lists\n",
    "\n",
    "# list comprehension that flattens nested list\n",
    "woman_sim_3 = [inner\n",
    "    for outer in woman_sim_3_nested\n",
    "        for inner in outer]\n",
    "\n",
    "woman_sim_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd level words similar to man_disc_2\n",
    "man_sim_3_nested = []\n",
    "for word in man_sim_2:\n",
    "    man_sim_3_nested.append(idx.similar_words(word)) # this returns nested lists\n",
    "\n",
    "# list comprehension that flattens nested list\n",
    "man_sim_3 = [inner\n",
    "    for outer in man_sim_3_nested\n",
    "        for inner in outer]\n",
    "\n",
    "man_sim_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops through woman similar and filters out words shared with man similar\n",
    "woman_disc_3 = []\n",
    "for word in woman_sim_3:\n",
    "    if word not in man_sim_3:\n",
    "        woman_disc_3.append(word)\n",
    "woman_disc_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops through woman similar and filters out words shared with man similar\n",
    "man_disc_3 = []\n",
    "for word in man_sim_3:\n",
    "    if word not in woman_sim_3:\n",
    "        man_disc_3.append(word)\n",
    "man_disc_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(woman_disc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woman_disc3_str = \" \".join(set(woman_disc_3))\n",
    "woman_disc3_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_disc3_str = \" \".join(set(man_disc_3))\n",
    "man_disc3_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
